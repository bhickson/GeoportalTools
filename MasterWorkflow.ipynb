{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master ingest workflow for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import sys\n",
    "import CSVtoISO19139\n",
    "import ISO19139toGBLjson\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import json\n",
    "import Utilities as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide password and file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile_metadata_file = utils.checkInput(\"Location of metadata CSV file on Sequoia:  \")\n",
    "dataset_location = utils.checkInput(\"Location of dataset: \")\n",
    "\n",
    "print(f\"\\nUsing metadata in file: {csvfile_metadata_file}\")\n",
    "print(f\"Using dataset: {dataset_location}\\n\")\n",
    "\n",
    "access_rights = utils.checkInput(\"provide access rights must be public or restricted: \",directory_exists=False) # 'public' or 'restricted' only\n",
    "\n",
    "geoserver_postgis_store = \"UA_Library_geospatialData\"\n",
    "geoserver_primary_workspace = \"UniversityLibrary\"\n",
    "metadata_repository_loc = \"../gitrepos/edu.uarizona/\"\n",
    "postgresql_pw = getpass(\"Provide password for PostgreSQL Database 'geo':\")\n",
    "geoserver_user = \"OGPAdmin\"\n",
    "geoserver_pw = getpass(f\"Provide password for GeoServer user {geoserver_user}:  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build dataset MD (csv -> xml)\n",
    "\n",
    "Coverts a csv file of metadata information into a metadata file in the ISO 19139 schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_files = CSVtoISO19139.csvtoISO(csvfile_metadata_file, dataset_location)\n",
    "renamed_ds = new_files[\"dataset\"]\n",
    "xml_file = new_files[\"metadata\"]\n",
    "base_file_name = os.path.basename(xml_file).split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Solr Metadata\n",
    "Creates a new search metadata file in the Geoblacklight schema and and moves into the metadata folder. The layerid is hashed using the fvn32 algorithm and then the hash is parsed out to create a folder structure (e.g. `UniversityLibrary:Arizona_CAPAqueduct_2002` -> `9ZQLzg7g5y` -> `9ZQ/Lzg/7g/5y/`.\n",
    "\n",
    "As well the xml file is copied to a new file (iso19139.xml and placed in the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoblacklightMD_dict = ISO19139toGBLjson.isoToGBL(metadata_repository_loc, xml_file, renamed_ds,\n",
    "                                                  rights=access_rights, \n",
    "                                                  institution=\"UArizona\",\n",
    "                                                  geoserver_workspace=\"UniversityLibrary\",\n",
    "                                                  tosolr=\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingest data to PostGIS DB\n",
    "\n",
    "For preview and subset download services from the geoportal, all vector data is ingested into a PostGIS database. New tables are placed in a schema named after their access permissions ('public' or 'restricted'). Raster files are not ingested, but read out of sequoia storage directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a tif or netCDF file (i.e. vector file) ingest to PostGIS\n",
    "if not renamed_ds.endswith(\".tif\") and not renamed_ds.endswith(\".nc\"):\n",
    "    epsg_code = utils.sendFileToPostGIS(renamed_ds, postgresql_pw, access_rights)\n",
    "else:\n",
    "    #epsg_code = input(\"Please provide epsg code for raster file\")\n",
    "    with rio.open(renamed_ds) as raster:\n",
    "        crs = raster.crs.to_string() # \"+init=epsg:26912\"\n",
    "        epsg_code = crs.split(\":\")[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Package Metadata (XML) & Data in ZIP\n",
    "The new xml file and now dataset are packaged in a zip file together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if shapefile, find all associate files (prj, dbf, sbx, etc...)\n",
    "if renamed_ds.endswith(\".shp\"):\n",
    "    shapefile_parent_dir = os.path.abspath(os.path.join(renamed_ds, os.pardir))\n",
    "    shapefile_fullname = os.path.basename(renamed_ds) # filename without path\n",
    "    shapefile_name = shapefile_fullname.split(\".\")[0] # everthing before extension\n",
    "    search_files = os.path.join(shapefile_parent_dir, shapefile_name) + \".*\"\n",
    "    datafiles = glob(search_files)\n",
    "elif renamed_ds.endswith(\".gpkg\") or renamed_ds.endswith(\".tif\") or renamed_ds.endswith(\".nc\"):\n",
    "    datafiles = [renamed_ds]\n",
    "else:\n",
    "    print(f\"Unknown datasets type for file {renamed_ds}. Must be shapefile, geopackage, netcdf, or geotiff.\")\n",
    "    raise ValueError\n",
    "\n",
    "#write list of files and xml metadata file to new zipfile\n",
    "zip_file_name = base_file_name + \".zip\"\n",
    "with ZipFile(zip_file_name, 'w') as myzip:\n",
    "    for file in datafiles:\n",
    "        myzip.write(file, os.path.basename(file))\n",
    "    myzip.write(xml_file, os.path.basename(xml_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Move to archival destination on sequoia\n",
    "The zip file is placed on sequoia in a directory corresponding to the originators name and the file name.  If desired, a customized directory can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequoia_loc = os.path.join(\"/sequoia/UAL_Vault/GeoArchive\", access_rights)\n",
    "originator_abbreviation = utils.checkInput(\"Provide dataset originator abbreviation with no spaces (e.g. USGS or UA_Libraries):\",  directory_exists=False)\n",
    "sequoia_loc = os.path.join(sequoia_loc, originator_abbreviation)\n",
    "\n",
    "sub_directories = base_file_name.split(\"_\")\n",
    "subdirectory = os.path.join(sub_directories[1], sub_directories[0], sub_directories[2])\n",
    "out_path = os.path.join(sequoia_loc, subdirectory)\n",
    "\n",
    "confirm = utils.checkInput(f\"Archive directory for {zip_file_name} is set to: {out_path}\\n Is this okay (Y or N or exit)?\", directory_exists=False)\n",
    "while confirm not in [\"Y\", \"YES\", \"y\", \"yes\", \"exit\"] :\n",
    "    out_path = utils.checkInput(\"Specify correct archive directory for file {}\", directory_exists=False)\n",
    "    confirm = utils.checkInput(f\"Archive directory for {zip_file_name} is set to: {out_path}\\n Is this okay (Y or N or exit)?\", directory_exists=False)\n",
    "    \n",
    "if confirm == \"exit\":\n",
    "    raise ValueError\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "zip_file_opath = os.path.join(out_path, zip_file_name)\n",
    "\n",
    "print(f\"Moving zip archive {zip_file_name} to directory {zip_file_opath} for storage\")\n",
    "shutil.move(zip_file_name, zip_file_opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Register file in iRods \n",
    "Now that the file is archived on sequoia, register the file location to iRods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only register in iRODS if dataset is a single vector or raster dataset. If an image pyramid (e.g. NAIP data). Register manually\n",
    "if not os.path.isdir(dataset_location):\n",
    "    irods_virtualized_location = f\"/UAL_dataZone/geospatial/{access_rights}/single_layer_datasets/\" + zip_file_name\n",
    "    ireg_command = \"ireg -V {} {}\".format(zip_file_opath, irods_virtualized_location)\n",
    "    print(\"Registering zip archive in irods\")\n",
    "    print(ireg_command)\n",
    "    os.system(ireg_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publish Layer in GeoServer\n",
    "Publish the layer in GeoServer through the GeoServer REST API. If a vector dataset, the layer is exposed through an exisiting PostGIS data store. If raster, we'll create the store for the tif file and publish the layer from the store.  In the case of raster datasets, they should have the data retiled (internall) for faster reading and [COG optimization](https://github.com/cogeotiff/cog-spec/blob/master/spec.md). For single geotiffs less than 40GB in size, generate internal overviews. Larger tiffs, or sets of files (e.g. NAIP imagery) should be served as a image pyramid, which will need to be optimized outside of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = (geoserver_user, geoserver_pw)\n",
    "headers = {'Content-Type': 'text/xml'}\n",
    "\n",
    "if renamed_ds.endswith(\".shp\") or renamed_ds.endswith(\".gpkg\"):\n",
    "    utils.postVectorLayer(base_file_name, epsg_code, geoserver_postgis_store, geoserver_primary_workspace, auth, headers)\n",
    "elif renamed_ds.endswith(\".tif\"):\n",
    "    #1. Create the Store in GeoServer to connect to data\n",
    "    # On the server geo, sequoia is mounted from the GeoArchive folder as sequoia\n",
    "    geo_relative_location = zip_file_opath.replace(\"/sequoia/UAL_Vault/GeoArchive\", \"/sequoia\")\n",
    "    utils.createGeoTiffDataStore(base_file_name, geoserver_primary_workspace, geo_relative_location, auth, headers)\n",
    "    \n",
    "    #2. Publish the layer from that store\n",
    "    utils.publishTiffLayer(base_file_name, geoserver_primary_workspace, epsg_code, auth, headers)\n",
    "else:\n",
    "    print(\"Unknown dataset store type. NetCDF?\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Push to github repository (unfinished)\n",
    "\n",
    "Search and dataset metadata that were built in step 2 were placed in the metadata repository which is also a github repo on OpenGeoMetadata. Commit the changes and push the new files and changes to layers.json. This section of code hasn't been tested yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(metadata_repository_loc)\n",
    "print(\"Adding all changed files...\")\n",
    "os.system(\"git add .\")\n",
    "\n",
    "commit_msg = \"\"\" \"Added metadata files for layer {}\" \"\"\".format(base_file_name)\n",
    "commit_cmd = \"git commit -m {}\".format(commit_msg)\n",
    "print(\"Commiting changes to github repository...\")\n",
    "os.system(commit_cmd)\n",
    "\n",
    "push_cmd = \"git push origin master\"\n",
    "print(\"Pushing changes to https://github.com/OpenGeoMetadata/edu.uarizona...\")\n",
    "os.system(push_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ingest to Solr\n",
    "\n",
    "Now that all data is ingested into archive and exposed through Geoserver and iRods, ingest the metadatafile to Solr to make discoverable in the portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solr_loc = \"http://geo.library.arizona.edu:8983/solr/UAL_GeospatialRecords\"\n",
    "solrURL = solr_loc + \"/update?commit=true\"\n",
    "# modify dict to be interpreted by Solr correctly\n",
    "solrDict = {\"add\": {\"doc\": geoblacklightMD_dict}}\n",
    "# turn python dictionary to json string\n",
    "solrString = json.dumps(solrDict, indent=4, sort_keys=False)\n",
    "# Set URL Put headers\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "print(\"Pushing record to Solr at {} ...\".format(solrURL))\n",
    "r = requests.post(solrURL, data=solrString, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Data to CrossRef (Ask Jeff)\n",
    "in XML file that gets generated\n",
    "get filepath from SDE Dataset URI (at the bottom)\n",
    "\n",
    "[https://doi.crossref.org/servlet/useragent](https://doi.crossref.org/servlet/useragent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
